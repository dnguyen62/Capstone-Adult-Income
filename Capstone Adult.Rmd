---
title: "Adult Income Distribution"
author: "Duong Nguyen"
date: "28 April 2019"
output: html_document
    
---
## Content

1. Executive Summary

2. Data

    2.1. Import and Cleaning
 
    2.2.	Exploration

3. Statistical Analysis

    3.1. Logistic Regression
 
    3.2. Model Estimation
 
    3.3. Output Reading
 
    3.4. Optimizing the Model
    
      3.4.1. Multicollinearity
           
      3.4.2. Deviance
    
   
    3.5. Interpretation of Model Estimates
 
    3.6. Finding the Right Threshold
 
    3.7. Validation
 
4. Summary and Conclusion




# 1.	 Executive Summary

From  the UCI machine learning repository website, the adult database was chosen. The dataset is a **panel**, i.e. at one point in time and across all respondents data were collected. The goal is to develop a model to **predict** whether someone likely earns **over 50k USD**.  For such a categorial question, the **logistic regression** was chosen. The best performing model is:

**Dependent variable**:    *income*

**Independent variables**: *age,  worklclass, education_num, occupation, relationship, race, sex, capital_gains, capital_loss, hours_per_week, native_region*

*omitted variables*:    *fnlwgt, education, marial_status*

The features someone has which make her/him likely to earn over 50k:

The person in question is not young. If she/he  works for the Federal Government, her/his odds is better than other people. The more time somebody has invested in her/his education the more likely this person will earn over 50k. If a person works in areas of Exec-managerial, Prof-specialty, Protective-service, Sales,  or Tech-support, her/his chances are high to have over 50k. 

As a husband the odds are higher than most people, except if someone is a working wife. In this case her chances are even higher to earn over 50k. If someone is not from any American-Indian-Eskimo minorities, his likelihood increases also. The person to earn over 50k is likely a male. If somebody has any capital gains or losses, they also indicate a high probability. 

If she/he works part-time, she/he is less likely to earn more than other people. And if someone is born from Central America, she/he is less likely to make 50k compared to others who are born in Western Europe or in the US.


The predicted value from the logistic regression is a probability of earning over 50k. For decision purpose, a **threshold for the probability** of **p=0.3** was chosen under considering the balance of true and false positive rates. 

The accuracy of the model using the train dataset results in *0.8254797*.

The accuracy of the model using the test dataset for validation results in **0.8269518**.



# 2.	Data
## 2.1.	 Import and Cleaning 

From  the UCI machine learning repository website, I chose the adult database for my capstone project. 
https://archive.ics.uci.edu/ml/datasets/Adult

Using a word editor or note pad,  it looks like as if the data are saved as text file separated by commas. After importing into R, I run through  the dataset to look for missing values. They are denoted as *" ?"*  The final code for importing the data looks like this:

```{r echo=FALSE, message=FALSE}
library(tidyverse)
library(caret)
library(dplyr)
library(ggplot2)
library(scales)
```
```{r}
#downloading dataset

adult <- read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data", 
                    header = FALSE, sep = ",", na=" ?")
dim(adult)

```

The dataset has a dimension of 32561 observations and 15 variables. 
As column names the following names are used: 

```{r}

#giving names for each column

names(adult) = c("age","workclass","fnlwgt","education","education_num","marial_status","occupation",
                 "relationship","race","sex","capital_gain","capital_loss","hours_per_week",
                 "native_country", "income")

```

The structure of the dataset is given by a mixture of integer and factor variables:

```{r}
#Structure of the dataset
str(adult)

```

All missing values are removed

```{r}

# ommitting all observations with nas and checking the new dimension
adult <- na.omit(adult)
dim(adult)

```

So over 2399 rows have been removed. (32561-30162).

The variable *fnlwgt* (final weighting) is a control variable for the data collection and will be removed from the dataset. 

```{r}

# variable fnlwgt will be dropped out, weighting variable
adult[[ "fnlwgt"]] <- NULL

```

The variable *age* is mapped into subcategories *Young, Middle_aged, Senior, and Old*:

```{r}

#mapping age of 15, 25, 45, 65, 100 into Young, Middle_aged, Senior, Old

adult[[ "age"]] <- ordered(cut(adult[[ "age"]], c(15,25,45,65,100)), 
                           labels= c("Young","Middle_age","Senior","Old"))
```

With regards to *education*, the factors are reordered:

```{r}

#education reordering
adult$education<-ordered(adult$education,levels=c(" Preschool"," 1st-4th"," 5th-6th",
                                                  " 7th-8th"," 9th"," 10th"," 11th"," 12th",
                                                  " HS-grad"," Some-college"," Assoc-acdm",
                                                  " Assoc-voc"," Bachelors"," Masters",
                                                  " Prof-school"," Doctorate"))

adult$education<-as.factor(adult$education)

```

The variable *hours_per_week* is mapped into *Part_time, Full_time, Over_time, Workaholic*:

```{r}

#mapping hours-per-week into Part_time, Full_time, Over_time, Workaholic

adult[[ "hours_per_week"]] <- ordered(cut(adult[[ "hours_per_week"]],
                                          c(0,25,40,60,110)),
                                      labels = c("Part_time", "Full_time", "Over_time", "Workaholic"))


```

*Capital gains* and losses are mapped into *None, Low, High* respectively:

```{r}

#mapping capital_gain

adult[[ "capital_gain"]] <- ordered(cut(adult[[ "capital_gain"]],
                                        c(-Inf,0,median(adult[[ "capital_gain"]][adult[[ "capital_gain"]]>0]),
                                          Inf)), labels = c("None", "Low", "High"))

#mapping capital_loss

adult[[ "capital_loss"]] <- ordered(cut(adult[[ "capital_loss"]],
                                        c(-Inf,0, median(adult[[ "capital_loss"]][adult[[ "capital_loss"]]>0]),
                                          Inf)), labels = c("None", "Low", "High"))


```

With regards to the column *native_country*, there are 41 countries that will be mapped into 8 *native_region*. Afterwards the column *native _country* will be removed.

```{r}

#native-country, there are 41 different countries
#mapping them into smaller groups
levels(adult$`native_country`)

# defining different regions
east_asia <- c(" Cambodia", " China", " Hong", " Laos", " Thailand",
               " Japan", " Taiwan", " Vietnam")

central_asia <- c(" India", " Iran")

central_america <- c(" Cuba", " Guatemala", " Jamaica", " Nicaragua", 
                     " Puerto-Rico",  " Dominican-Republic", " El-Salvador", 
                     " Haiti", " Honduras", " Mexico", " Trinadad&Tobago")

south_america <- c(" Ecuador", " Peru", " Columbia")


west_europe <- c(" England", " Germany", " Holand-Netherlands", " Ireland", 
                 " France", " Greece", " Italy", " Portugal", " Scotland")

east_europe <- c(" Poland", " Yugoslavia", " Hungary")

adult <- mutate(adult, 
                native_region = ifelse(native_country %in% east_asia, " East-Asia",
                                       ifelse(native_country %in% central_asia, " Central-Asia",
                                              ifelse(native_country %in% central_america, " Central-America",
                                                     ifelse(native_country %in% south_america, " South-America",
                                                            ifelse(native_country %in% west_europe, " Europe-West",
                                                                   ifelse(native_country %in% east_europe, " Europe-East",
                                                                          ifelse(native_country == " United-States", " United-States", 
                                                                                 " Others" ))))))))


adult$native_region <- factor(adult$native_region, ordered = FALSE)

# dropping native_country column, as it has become absolete

adult[[ "native_country"]] <- NULL

```

The final structure of the modified dataset looks like this:

```{r}
# Checking the final structure of the dataset
str(adult)
```

The dataset given is a panel, i.e. at one point in time and across all respondents data were collected. If the same respondents were examined over a long period, then we would have a time series.


## 2.2.	Exploration

The following charts shows over different variables how the income distribution is split:

```{r}

# Percentage distribution of income across different occupations

adult %>% count(occupation, income) %>% 
  left_join(adult %>% count(occupation), by="occupation") %>% 
  mutate(pct=(n.x/n.y)*100, ypos=0.6*n.x) %>% 
  ggplot(aes(x=reorder(occupation,n.x), n.x, fill=income)) +
  geom_bar(stat = "identity", width = 0.5) +
  geom_text(position="stack",aes(label=paste0(sprintf ("%1.1f", pct),"%"), y=ypos))+
  coord_flip()+
  labs(title = "Income Split Within Different Occupations ")

```


The chart *Income Split Within Different Occupations* shows the income distribution over different occupations. The three occupations with the most respondents are *Prof-specialty, Craft-repair,* and *Exec-managerial*. The occupations with the least respondents are *Protective-serv, Priv-house-serv,* and *Armed-Forces*. The three occupations with the highest proportions of people with income over 50k are *Exec-managerial, Prof-specialty*, and  *Protective-serv*.

```{r}
#Distribution across occupations and sex

adult %>% count(occupation, income, sex) %>% 
  left_join(adult %>% count(occupation), by="occupation") %>% 
  ggplot(aes(x=reorder(occupation,n.x), n.x, fill=sex)) +
  geom_bar(stat = "identity") +
  facet_grid(cols = vars(income))+
  coord_flip()+
  labs(title = "Income Split Across Occupations And Sex")

```

The chart *Income Split Across Occupations And Sex* outlines the dominance of male in having incomes over 50k. Most women earn less than 50k. A high number of women are working in *Adm-clerical, Other-service*, and *Sales*. It also shows that most respondents are male.

```{r echo=FALSE}

#Distribution across occupations and age

adult %>% count(occupation, income, age) %>% 
  left_join(adult %>% count(occupation), by="occupation") %>% 
  ggplot(aes(x=reorder(occupation,n.x), n.x, fill=age)) +
  geom_bar(stat = "identity") +
  facet_grid(cols = vars(income))+
  coord_flip()+
  labs(title = "Income Split Across Occupations And Age")

```

The chart *Income Split Across Occupations and Age* describes that young people are less likely to earn over 50k.

```{r echo= FALSE}

#Distribtuion across age and sex
adult %>% count(age, income, sex) %>% 
  left_join(adult %>% count(age), by="age") %>% 
  ggplot(aes(x=reorder(age,n.x), n.x, fill=sex)) +
  geom_bar(stat = "identity") +
  facet_grid(cols = vars(income))+
  coord_flip()+
  labs(title = "Income Split Across Age and Sex")

```

The chart *Income Split Across Age and Sex* shows the majority of respondents are in the groups of *Middle_age* and *Senior*. These two groups also have the highest number of people with income over 50k.

Regarding gender, the proportion of female and male in the *Young* age group is fairly even. But within the groups of *Middle_age* and *Senior* this proportion changes in favour of male. 

```{r echo=FALSE}

#Distribution across marial_status and sex
adult %>% count(marial_status, income, sex) %>% 
  left_join(adult %>% count(marial_status), by="marial_status") %>% 
  ggplot(aes(x=reorder(marial_status,n.x), n.x, fill=sex)) +
  geom_bar(stat = "identity") +
  facet_grid(cols = vars(income))+
  coord_flip()+
  labs(title = "Income Split Across Marial Status and Sex")
```

Could it be that because most married women decide to stay at home or choose to work part-time?

The chart *Income Split Across Marrial Status* reveals that the group *Married-civ-spouse* is dominated by *Male*. The male members of that group also have the highest numbers of people earning over 50k. The second largest group is the *Never-married* group. The proportion of female and male is quite even. It could be that this group consists mostly of young people. 

```{r echo=FALSE}

#Distribution across work hours_per_week and sex
adult %>% count(hours_per_week, income, sex) %>% 
  left_join(adult %>% count(hours_per_week), by="hours_per_week") %>% 
  ggplot(aes(x=reorder(hours_per_week,n.x), n.x, fill=sex)) +
  geom_bar(stat = "identity") +
  facet_grid(cols = vars(income))+
  coord_flip()+
  labs(title = "Income Split Across Work Hours Per Week and Sex")

```

The chart *Income Split Across Work Hours Per Week and Sex* reveals that most women work full-time and over-time. Within the part-time jobs the proportion between female and male is quite even. I would have expected it to be dominated by females. Also here the females that choose to stay at home are not recognized.

```{r echo=FALSE}

#Distribution across relationship and sex
adult %>% count(relationship, income, sex) %>% 
  left_join(adult %>% count(relationship), by="relationship") %>% 
  ggplot(aes(x=reorder(relationship,n.x), n.x, fill=sex)) +
  geom_bar(stat = "identity") +
  facet_grid(cols = vars(income))+
  coord_flip()+
  labs(title = "Income Split Relationship and Sex")

```

In the chart *Income Split Relationship and Sex*, the dominant group is *Husband*. But whom they are married to? 
The number of *Wife* is fairly small in the panel. It looks like married women decide to stay home. Looking at the groups *Not-in-family, Own-child, Unmarried*, and *Other relative*, the proportion of females and males are quite even. The group *Unmarried* shows a dominance of females.

```{r echo=FALSE}

#Distribution across education and sex
adult %>% count(education, income, sex) %>% 
  left_join(adult %>% count(education), by="education") %>% 
  ggplot(aes(x=education, n.x, fill=sex)) +
  geom_bar(stat = "identity") +
  facet_grid(cols = vars(income))+
  coord_flip()+
  labs(title = "Income Split Across Education and Sex")

```

The chart *Income Split across Education and Sex* shows the majority of respondents have a *HS-grad, Some-college*, or *Bachelors*. 
People earning over 50k have at least a *HS-grad*. The higher the education , the higher proportion of people having earnings over 50k. Again, males dominate in every aspects.

```{r echo= FALSE}

# Education in number of years
adult %>% count(education_num, income) %>% 
  left_join(adult %>% count(education_num), by="education_num") %>% 
  mutate(pct=(n.x/n.y)*100, ypos=0.5*n.x) %>% 
  ggplot(aes(x=education_num, n.x, fill=income)) +
  geom_bar(stat = "identity", width = 0.5) +
  geom_text(position="stack",aes(label=paste0(sprintf ("%1.1f", pct),"%"), y=ypos))+
  scale_y_log10()+
  coord_flip()+
  labs(title = "Income Split Within Different Education Years ")

```

Another view on education provides the variable *education_num*. The chart *Income Split Within Different Education Years* demonstrates that the percentage of people earning over 50k are higher with more years of education spent. Note the horizontal axis is in log scale.

```{r echo=FALSE}

#Distribution across workclass and sex
adult %>% count(workclass, income, sex) %>% 
  left_join(adult %>% count(workclass), by="workclass") %>% 
  ggplot(aes(x=reorder(workclass,n.x), n.x, fill=sex)) +
  geom_bar(stat = "identity") +
  facet_grid(cols = vars(income))+
  coord_flip()+
  labs(title = "Income Split Across Workclass and Sex")
```

The chart *Income Split Across Workclass and Sex* reveals that most respondents are employed in the private sector. 

```{r echo=FALSE}
#Distribution Within workclasses
adult %>% count(workclass, income) %>% 
  left_join(adult %>% count(workclass), by="workclass") %>% 
  mutate(pct=(n.x/n.y)*100, ypos=0.5*n.x) %>% 
  ggplot(aes(x=reorder(workclass,n.x), n.x, fill=income)) +
  geom_bar(stat = "identity", width = 0.5) +
  geom_text(position="stack",aes(label=paste0(sprintf ("%1.1f", pct),"%"), y=ypos))+
  scale_y_log10()+
  coord_flip()+
  labs(title = "Income Split Within Different Workclasses ")


```

But the proportion of people earning over 50k is quite high, if they are self-employed or working in governmental sector, as outlined in the chart *Income Split Within Different Workclasses*. Note the horizontal axis is in log scale.



```{r echo=FALSE}

# Income Split across races

adult %>% count(race, income) %>% 
  left_join(adult %>% count(race), by="race") %>% 
  mutate(pct=(n.x/n.y)*100, ypos=0.5*n.x) %>% 
  ggplot(aes(x=reorder(race,n.x), n.x, fill=income)) +
  geom_bar(stat = "identity", width = 0.5) +
  geom_text(position="stack",aes(label=paste0(sprintf ("%1.1f", pct),"%"), y=ypos))+
  scale_y_log10()+
  coord_flip()+
  labs(title = "Income Split Within Different Races ")

```

The dominant race in the panel is white. That is why the scale of the chart *Income Split Within Different Races* is in log scale. The income split within the *White* and *Asian-Pac-Islander* groups are fairly similar (26% to 74%). In contrast, the split within the *Black, Amer-Indian-Eskino*, and *Other* group is far more to 10% to 90%. 

```{r echo=FALSE}

# $Income Split across Native_regions

adult %>% count(native_region, income) %>% 
  left_join(adult %>% count(native_region), by="native_region") %>% 
  mutate(pct=(n.x/n.y)*100, ypos=0.5*n.x) %>% 
  ggplot(aes(x=reorder(native_region,n.x), n.x, fill=income)) +
  geom_bar(stat = "identity", width = 0.5) +
  geom_text(position="stack",aes(label=paste0(sprintf ("%1.1f", pct),"%"), y=ypos))+
  scale_y_log10()+
  coord_flip()+
  labs(title = "Income Split Within Different Native Regions ")

```

Depending where the respondents were born, there are differences in the proportion of the people earning over 50k. The groups *South-America* and *Central-America*  have the lowest proportions of people with income over 50k compared to the group *United-States*. Whereas the groups *Central Asia, Europe-West, East-Asia*, and *Others* have higher proportions of high income earners. Note the horizontal axis is in log scale.

```{r echo=FALSE}
#capital gain
adult %>% count(occupation, income, capital_gain) %>% 
  left_join(adult %>% count(occupation), by = "occupation" )%>% 
  ggplot(aes(x=reorder(occupation,n.x), n.x, fill=capital_gain)) +
  geom_bar(stat = "identity") +
  facet_grid(cols = vars(income))+
  coord_flip() +
  labs(title = "Income Split Across Occupations and Capital Gains")

```

The chart *Income Split Across Occupations and Capital Gains* shows that most people do not have any capital gains. The few that have high capital gains are also likely to earn over 50k. They are the ones who can put some money for investments.

```{r echo=FALSE}
#capital loss
adult %>% count(occupation, income, capital_loss) %>% 
  left_join(adult %>% count(occupation), by = "occupation" )%>% 
  ggplot(aes(x=reorder(occupation,n.x), n.x, fill=capital_loss)) +
  geom_bar(stat = "identity") +
  facet_grid(cols = vars(income))+
  coord_flip() +
  labs(title = "Income Split Across Occupations and Capital Losses")

```

The chart *Income Split Across Occupations and Capital Losses* indicates like the chart before, that most people do not have any capital losses. The ones who earn more 50k, are likely to have capital loss, as they have money for investing.



# 3.	Statistical Analysis

## 3.1. Logistic Regression

As mentioned before the dataset given is a panel with the dependent variable *income* as discrete variable. The task of the analysis is to predict whether someone earns over 50k. For such categorical issue the logistic regression approach is well suited.


The general idea of the approach is to give a probability if someone earns more than 50k based on the input of the dependent variables (note: *" >50k"*is at higher level in R). 

$$Pr ( income >50k | Independent Variables ) = p$$ 
where $$p=\frac{1}{1+exp(-y)}$$

$$y = \beta_0 + \beta_1*X_1 + \beta_2*X_2 +..+ error$$
$$X_1, X_2, ... Independent Variables$$

The probability function *p* has a s-shape curve and is bounded between 0 and 1. For illustration the probability function could look like this:

```{r echo=FALSE, fig.height = 4, fig.width = 5 }
# Illustration of logit function

x_test <- seq(1:100)
y_test <- 1/(1+exp((-1)*(-5+(0.10)*x_test)))
plot(x_test, y_test)
```

The graph suggests that the higher the x value the higher the probability of y. In this example, low x values (<40) implies  y=0, and high x values (>60) implies y=1. What if we have observations with low x values but with y=1 or vice versa? So the shape of the probability function has to change. It could be flat, gradually increasing, or steep.

Given the distribution of the *income* variable, the logistic approach tries to find the one shape of the probability curve that can cover most of the observations of the variable *income*. The optimization method behind is the maximum likelihood estimation. 

We are interested in p, the probability or earning over 50k. For estimation purpose the probability function will be transformed into a log odds function, i.e

$$ln(\frac{p} {1-p}) = \beta_0 + \beta_1*X_1 + \beta_2*X_2 + ..+ error$$
(Reference: A nice illustrative introduction into logistic regression and maximum likelihood could be found here: [link](https://www.youtube.com/watch?v=vN5cNN2-HWE))


#3.2.  Model Estimation

Before proceeding the ordered variables *age, education, capital_gains, capital_loss, hours_per_week* will be changed into unordered factors. The dataset *adult2* is copy of *adult* but with unordered factors.( Otherwise, the output will show for ordered factors in x.L, x.Q, x.C (linear, quadratic, cubic parameters)).

```{r}
#changing ordered variables age,education, capital_gains, capital_loss, hours_per_week into unordered factors

adult2 <- adult
adult2$age <- factor(adult2$age, ordered = FALSE)
adult2$education <- factor(adult2$education, ordered = FALSE)
adult2$capital_gain <- factor(adult2$capital_gain, ordered = FALSE)
adult2$capital_loss <- factor(adult2$capital_loss, ordered = FALSE)
adult2$hours_per_week <- factor(adult2$hours_per_week, ordered = FALSE)

```

The dataset will be split into a *train_set* and *test_set*. The train set will be used for estimation and optimization, while the test set will be only used for validation.
80% of the data will be used for training, while 20% will be for testing.

```{r echo=FALSE, message= FALSE}
# creating a train and test set
library(caret)
set.seed(755)
```
```{r}
test_index <- createDataPartition(y = adult2$income, times = 1, p = 0.2, list = FALSE)
train_set <- adult2[-test_index,]
test_set <- adult2[test_index,]
```

The first run of the logistic regression will be with all independent variables:

**Dependent variable**:   *income*

**Independent variable**: *age,  worklclass,education, education_num, marial_status, occupation, relationship,                           race, sex, capital_gains, capital_loss, hours_per_week, native_region*

```{r}
# logistic regression
# first run

glmfit <- glm(income~., data=train_set, family=binomial)

summary(glmfit)

```

##3.3. Output Reading

Above is the output of the logistic regression income versus all other variables.
The first column *coefficient* indicates the name of each variables used. The second column shows the estimates for each coefficient. 

Surprisingly, there are more coefficients than the number of independent variables. R looks at the different levels of each variables. 

For example the variable *sex* has two levels, i.e. *female* and *male*. As reference to female, the coefficient estimate of 0.864354 means that relative to a female the odds for an income >50k increases with male. Note, the estimation is in a *ln(p/1-p)* world. The more positive the number the higher the odds.

Let`s have a look at the variable *relationship* for example. The one missing level is *Husband*. Relative to someone who is husband, if someone is *Not-in-family*, his or her odds of having an income >50k increases. Similarly, relative to a husband, if someone is a wife, the odds that this woman earns over 50k is high. 

Quite strange when considering that most women in the panel earn less than 50k. But it could be, that married women who decide to work, choose to do so, because they can make over 50k.

On the far right column, the p-value for all coefficients are listed. Next to them are either several stars, or nothing.

The three stars $***$ means that with 99.90% confidence level, the respective coefficient is statistically significant different from zero. ($**$ = 99.00%; $*$= 95% ; . = 90%). Put differently, the variables associated with the significant coefficients do have influence on the outcome of the variable income.


It looks like that *education* seems to be insignificant. Whereas with other variables, there are some or all levels that are significant. And with regards to the variable *education_num* there are **NAs** in the output.

Intuitively one would expect that education should have a significant influence whether someone earns over 50k. As shown in the data exploration part, there should be some correlation between education and the probability of earning over 50k. But the regression result does not support this thesis.


(Reference: A illustrative example how to run the logistic regression and read the output, could be be found here: [link](https://www.youtube.com/watch?v=AVx7Wc1CQ7Y) )


## 3.4.  Optimizing the Model

### 3.4.1. Multicollinearity	

Multicollinearity exists if the independent variables are not independent from each other. The estimates of the model will have high standard errors resulting  into insignificant estimates. 
(Reference: [link](http://scg.sdsu.edu/logit_r/)). 

My suspicion is that the variables *education* and *education_num* are highly correlated.

In the second run, the variable education will be removed

```{r, size="small"}
# first run with variables results into NA estimates for education_num, 
# my suspiscion is education and education_num are highly correlated
# new run without education

glmfit <- glm(income~.-education, data=train_set, family=binomial)
summary(glmfit)

```

The new output shows no **NAs** for *education_num*. 

I also omit the *education_num* variable, and keep *education* instead. The result shows no significance for all level coefficients of *education*, which does not make sense. So the preferred model includes the variable *education_num*.

To detect for further mulitcollinaerity, the function *vif()* will be used. Variables with values over 5 exhibit correlation with each other.

```{r echo=FALSE, message= FALSE}

# Are any other multicollinearities among the independent variabels?
# car package; test of multicollinearity of independent variabels
# reference: http://scg.sdsu.edu/logit_r/

library(car)
```
```{r}
vif(glmfit)  

```

The output of the *vif()* function indicates that *marial_status* and *relationship* are highly correlated. The next runs will be one without *relationship* and one without *marial_status*.

```{r}
# attempt to eliminate variables with a GVIF higher than 5, 
# the variables marial_status and relationship do have GVIF values >5.
# regression without variable relationship

glmfit <- glm(income~.-education-relationship, data=train_set, family=binomial)
summary(glmfit)

vif(glmfit) # all remaining independent variables no longer correlated

# regression without variable marial_status
# residual deviance and AIC number are worse than witout marial_status instead
# all remaing variable independent variables no longer correlated 

glmfit <- glm(income~.-education-marial_status, data=train_set, family=binomial)
summary(glmfit)

vif(glmfit) # all remaining independent variables no longer correlated
```

In both regression, the *vif()* function no longer shows any multicollinearity issues. But which model is best? 

### 3.4.2. Deviance

On the bottom of the output of the logistic regression, the numbers for null deviance, residual deviance, and the AIC are given. 

These numbers show how well the model fits to the data. The lower the numbers the better the model. The null deviance number shows how well the model fits with only the intercept. The residual deviance number and AIC measure the fitness with inclusion of the independent variable.

(Reference: What deviance means, could be found here: [link](https://www.youtube.com/watch?v=B2nJ3U4E1VA))

The model without the variable *relationship* shows the following deviance and AIC numbers:

  *Residual deviance: 15613  on 24080  degrees of freedom*
  
  *AIC: 15711*

The model without the variable *marial_status* has the following deviance and AIC numbers:

  *Residual deviance: 15513  on 24081  degrees of freedom*
  
  *AIC: 15609*

The model without *education* and *marial_status* is the best performing model. The residual deviance and the AIC number are less than the model with relationship instead.

The final model looks like this:

**Dependent variable**:   *income*

**Independent variable**: *age,  worklclass, education_num,  occupation, relationship, race, sex, capital_gains, capital_loss, hours_per_week, native_region*

**Omitted variabels**: *fnlwgt, education, marial_status*


 
## 3.5 Interpretation of Model Estimates

In plain English, the features someone has which makes him likely to earn over 50k:

You are no longer a young person. If you work for the Federal Government, your odds is better than other people. The more time you have invested in your education the more likely you will earn over 50k.

If you work in areas of Exec-managerial, Prof-specialty, Protective-service, Sales,  or Tech-support, your chances are high to have over 50k. 
As a husband your odds are higher than most people, except if you are a working wife. In this case your chances are even higher to earn over 50k. 

If you are not from any American-Indian-Eskimo minorities, the likelihood increases also. You are likely a male. If you have any capital gains or losses also indicates a high probability. If you work part-time, you are less likely to earn more than other people. 

And if you are born from Central America, you are less likely to make 50k compared to others who are born in Western Europe or in the US.

## 3.6.  Finding the Right Threshold

Recall that the estimates were obtained in the *ln(p/1-p)* world. Using the estimates from that regression,  the *predict()* function returns a vector of probabilities. 

Intuitively, one would say that a probability over 0.5 could be a good indication if someone earns over 50k.  

The table below shows for the predicted value *FALSE*, if someone earns less than 50k. If someone earns more than 50k, then the predicted value is *TRUE*.

```{r}
# proposed probabilities given estimates from logistic model
glm_hat <- predict(glmfit, data = train_set, type = "response")


# As threshold whether someone earns over 50k, a probability of 0.5 is assumed
table(ActualValue=train_set$income, PredictedValue = glm_hat >0.5)

```

The accuracy is $(16866+3635)/(16866+1257+2371+3635) = 0.8496415$.

Could the accuracy be improved by altering the threshold number? 

That is to find the threshold number, that increases the number that the model predicts correctly. Or put differently the numbers in the confusion matrix along the down diagonal, i.e.  (16847 and 3667) from left to right should be increased (true positive). 
Unfortunately there is a cost, the numbers of the false predictions increases as well (false positive rate). There is a trade off the be considered:

```{r echo=FALSE, message=FALSE}

#finding threshold number; package ROCR
library(ROCR)
```
```{r}
ROCRPred = prediction(glm_hat, train_set$income)
ROCRPerf <- performance(ROCRPred, "tpr","fpr")
plot(ROCRPerf, colorize=TRUE  , print.cutoffs.at=seq(0.1, by=0.1))

```

The chart plots the true positive rate against the false positive rate. 

As we move along the curve, we decrease the threshold value *p*. The true positive rate increases sharply at the beginning, but the incremental gain diminishes more and more and the "cost" in form of the false positive increases sharply.

The optimum should be the one threshold value *p*, where one unit gain in true positive rate equals one unit in false positive rate lost.

From the chart above, a threshold value of 0.3 looks reasonable. The resulting confusion matrix looks like this:

```{r}
# From the ROCR plot, the best ration is around 0.3

table(ActualValue=train_set$income, PredictedValue = glm_hat >0.3)


```

The accuracy is given by $(15082+4836)/(15082+3041+1170+4836) = 0.8254797$.

The accuracy decreases with a *p* value of *0.30*. Although the accuracy is less than before, the threshold of 0.30 has a more balanced consideration of the true positive rate and false positive rate.

(Reference: [link](https://www.datacamp.com/community/tutorials/confusion-matrix-calculation-r))


## 3.7. Validation

Using the dataset *test_set* for validation, the result looks as follows:

```{r}
#  validation with test dataset

glm_hat_test <- predict(glmfit, newdata = test_set, type = "response")

table(ActualValue=test_set$income, PredictedValue = glm_hat_test >0.3)


``` 

The accuracy of the model using the *test_set* dataset results $(3764+1225)/(3764+767+277+1225)= 0.8269518$.


# 4. Summary and Conclusion

From  the UCI machine learning repository website, the panel dataset adult was chosen. The goal was to develop a model to predict whether someone was likely to earn over 50k USD.  For such a categorial question, the logistic regression was chosen. After eliminating multicollinearity issues, the best performing model is as follows:

**Dependent variable**   :  *income*

**Independent variables**: *age,  worklclass, education_num, occupation, relationship race, sex, capital_gains,                            capital_loss, hours_per_week, native_region*


*omitted variables*:    *fnlwgt, education, marial_status*

The features someone has which make her/him likely to earn over 50k:

The person in question is not young. If she/he  works for the Federal Government, her/his odds is better than other people. The more time somebody has invested in her/his education the more likely this person will earn over 50k. If a person works in areas of Exec-managerial, Prof-specialty, Protective-service, Sales, or  Tech-support, her/his chances are high to have over 50k. 

As a husband the odds are higher than most people, except if someone is a working wife. In this case her chances are even higher to earn over 50k. If someone is not from any American-Indian-Eskimo minorities, his likelihood increases also. The person to earn over 50k is likely a male. If somebody has any capital gains or losses, they also indicate a high probability. 

If she/he works part-time, she/he is less likely to earn more than other people. And if someone is born from Central America, she/he is less likely to make 50k compared to others who are born in Western Europe or in the US.


The predicted value from the logistic regression is a probability of earning over 50k. For decision purpose, a threshold for the probability of *p=0.3* was chosen under considering the balance of true and false positive rates. 

The accuracy of the model using the train dataset results in *0.8254797*.

The accuracy of the model using the test dataset for validation results in *0.8269518*.

